\chapter{Implementation}
\label{chp:implementation} 

\section{Introduction}

\section{ROS Integration}

Implementation procedure for mobile robot:

\begin{itemize}

	\item Decide on ROS message interface.
	\item Write interfaces for the motor drivers.
	\item Create a description of the physical structure and properties of the robot in \ac{URDF}. 
	\item Extend the model to enable simulation in Gazebo.
	\item Publish coordinate transform data via \textit{tf} and visualize it in rviz.
	\item Add sensors, with driver and simulation support.
	\item Apply algorithms for navigation and other functionality. 

\end{itemize}

\section{Modeling}

\subsection{Introduction}



\subsection{Physical Dimensions}

The inertia tensor:

\begin{equation}
    	I = \begin{bmatrix}
    	I_{xx} & I_{xy} & I_{xz} \\[0.3em]
    	I_{yx} & I_{yy} & I_{yz} \\[0.3em]
    	I_{zx} & I_{zy} & I_{zz}
    	\end{bmatrix}
\end{equation}

Inertia tensor for a solid, uniform cylinder where the radius $r$ is measured in parallel to the $x - y$ plane, and $h$ is parallel to the $z$ axis:
\begin{equation}
I_{cylinder} = \frac{1}{12}m \begin{bmatrix}
	(3 r^2 + h^2) & 0 & 0 \\[0.3em]
	0 & (3 r^2 + h^2) & 0 \\[0.3em]
	0 & 0 & r^2
	\end{bmatrix}
\end{equation}

Inertia tensor for a solid, uniform cuboid. The subscript of $l$ indicates which axis $l$ is measured along:
\begin{equation}
I_{cuboid} = \frac{1}{12}m \begin{bmatrix}
	(l_y^2 + l_z^2) & 0 & 0 \\[0.3em]
	0 & (l_x^2 + l_z^2) & 0 \\[0.3em]
	0 & 0 & (l_x^2 + l_y^2)
\end{bmatrix}
\end{equation}

\subsection{Coordinate Frames}

\begin{figure}[p]
	\centering
	\includegraphics[width=0.5\textwidth]{sensor_frames}
	\caption{Robot model with frames for laser, Kinect, robot base and map. }
	\label{fig:sensor_frames}
\end{figure}

\begin{figure}[p]
	\centering
	\includegraphics[width=0.5\textwidth]{sensors_in_frame}
	\caption{Sensor input placed with correct transformations from \textit{base\_link}. }
	\label{fig:sensors_in_frame}
\end{figure}


\section{Simulations}

Robot simulation was done in Gazebo, a simulation tool with good interfaces to \ac{ROS}. The same \ac{ROS} graph was used for both the simulated and real version of the robot, except from the sensors and actuators, and some minor parameter changes. 

\section{Motion Control}

\section{ROS Nodes for Motion Control}


\subsection{Velocity Command Sources}

There are four ways to control the robot:

\begin{itemize}
	\item Local keyboard input.
	\item Wireless teleoperation from the \ac{OCS}.
	\item Wireless teleoperation from a handheld Bluetooth device.
	\item Commands from the navigation stack in \ac{ROS}.
\end{itemize}

\begin{figure}[p]
	\centering
	\includegraphics[width=1\textwidth]{velocity_command}
	\caption{Nodes and topics for motion control. }
	\label{fig:move_base_nodes}
\end{figure}

\begin{figure}[p]
	\centering
	\includegraphics[width=1\textwidth]{velocity_ramp_cropped}
	\caption{Velocity command ramping. The blue line represents commands entering \textit{velocity\_ramp}, while the red line shows the acceleration constrained output command.}
	\label{fig:velocity_ramp}
\end{figure}

\subsection{Motor Control Card Firmware on XMEGA A3BU}

XMEGA A3BU is an evaluation board developed by Atmel. The board has been used in earlier projects on this robot. The implementation presented here, is based on Petter Aspunviks implementation \cite{aspunvik}. The firmware will now receive velocity commands based on the \texttt{geometry\_msgs/Twist} message format in \ac{ROS}, and translate these into the command format used by each motor. Speed settings for each motor is based on \ac{PWM}. 

There were two requirements for this implementation: 
\begin{enumerate}
\item When velocity commands from the operating system are either absent or incomplete, the robot shall stop.
\item The program shall translate linear and angular velocity commands into wheel commands.
\end{enumerate}

%Sett inn koblingsskjema her!

The connections are the same as in \cite{aspunvik}, except for the installation of more secure connections under the robot. The old connections were insecure, and the risk of short circuits was substantial.

\section{Operator Control Station (OCS)}

The \ac{OCS} allows an operator to control and monitor the robot through a graphical user interface. \texttt{MainWindow}

\subsection{Graphical User Interface}

A Qt-based \ac{GUI}...

\section{The Hand Held Remote Control - \textit{Robot Leash}}

Because the \ac{OCS} is only partially implemented, an operator will not have access to all the features on the robot. In addition, as a safety precaution a person should be close to the robot at all times, and be ready to pull the plug. Furthermore, it is hard to control a moving robot through the on-board keyboard. These problems were countered by the Android-based remote control, \textit{Robot Leash}. 


\begin{figure}
	\centering
	\begin{subfigure}[b]{0.30\textwidth}
		\includegraphics[width=\textwidth]{device_select}
		\caption{First activity with device list.}
		\label{fig:device_select}
	\end{subfigure}
		\begin{subfigure}[b]{0.30\textwidth}
			\includegraphics[width=\textwidth]{bt_request}
			\caption{The user is prompted to pair with the robot.}
			\label{fig:bt_request}
		\end{subfigure}
	\begin{subfigure}[b]{0.30\textwidth}
		\includegraphics[width=\textwidth]{using_app}
		\caption{Controlling the robot with the stick.}
		\label{fig:using_app}
	\end{subfigure}
	\caption{\label{fig:app_screens}A typical use case for ''Robot Leash''.}
\end{figure}

\subsection{Connecting to the Robot}

\begin{enumerate}
	\item The first screen after scanning for devices. There is no device filtering, and the user can select any device, but only connect through a specific service.
	\item After selecting a device which provides the correct service, the user will be prompted to pair the devices.
	\item The smartphone and the robot is now paired, and velocity commands from the blue control stick are passed to the robot via Bluetooth.
\end{enumerate}

http://developer.samsung.com/technical-doc/view.do?v=T000000117

\section{Mapping}

\section{Navigation}

\subsection{Global Path Planning}

\subsection{Local Path Planning}

\subsubsection{Obstruction Detection}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.53\textwidth}
		\includegraphics[width=\textwidth]{3d_obstruction1_left}
		\caption{A point cloud representation of the obstruction. Notice how the local costmap is based on the detected point cloud.}
		\label{fig:device_select}
	\end{subfigure}
		\begin{subfigure}[b]{0.45\textwidth}
			\includegraphics[width=\textwidth]{3d_obstruction1_right}
			\caption{The obstruction in the Gazebo simulator. Notice how the LIDAR only detects the wheels below the container.}
			\label{fig:bt_request}
		\end{subfigure}
	\caption{\label{fig:3d_obstruction1}Detecting obstructions in 3d.}
\end{figure}